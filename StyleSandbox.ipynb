{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleSandbox",
      "provenance": [],
      "authorship_tag": "ABX9TyPaFljhFiH/Jsb+99C0Wuhr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balcomes/keras-autoencoders/blob/master/StyleSandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ21cHx-koz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26d7287a-020e-4923-f6e3-54824521fe41"
      },
      "source": [
        "import numpy as np\n",
        "import imageio\n",
        "import time\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "\n",
        "target_image_path = 'sprite.png'\n",
        "style_reference_image_path = 'dwarves.png'\n",
        "width, height = load_img(target_image_path).size\n",
        "img_height = 400\n",
        "img_width = int(width * img_height / height)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  img = load_img(image_path, target_size=(img_height, img_width))\n",
        "  img = img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  img = vgg19.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1]\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x\n",
        "\n",
        "target_image = K.constant(preprocess_image(target_image_path))\n",
        "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
        "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
        "\n",
        "input_tensor = K.concatenate([target_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)\n",
        "\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet',\n",
        "                    include_top=False)\n",
        "\n",
        "print('Model loaded.')\n",
        "\n",
        "def content_loss(base, combination):\n",
        "  return K.sum(K.square(combination - base))\n",
        "\n",
        "def gram_matrix(x):\n",
        "  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "  gram = K.dot(features, K.transpose(features))\n",
        "  return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "  S = gram_matrix(style)\n",
        "  C = gram_matrix(combination)\n",
        "  channels = 3\n",
        "  size = img_height * img_width\n",
        "  return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "def total_variation_loss(x):\n",
        "  a = K.square(x[:,:img_height-1,:img_width-1,:] - x[:,1:,:img_width-1,:])\n",
        "  b = K.square(x[:,:img_height-1,:img_width-1,:] - x[:,:img_height-1,1:,:])\n",
        "  return K.sum(K.pow(a + b, 1.25))\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "\n",
        "content_layer = 'block5_conv2'\n",
        "\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "\n",
        "total_variation_weight = 1e-4\n",
        "style_weight = 1.\n",
        "content_weight = 0.025\n",
        "loss = K.variable(0.)\n",
        "layer_features = outputs_dict[content_layer]\n",
        "target_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "\n",
        "loss += content_weight * content_loss(target_image_features,\n",
        "                                      combination_features)\n",
        "\n",
        "for layer_name in style_layers:\n",
        "  layer_features = outputs_dict[layer_name]\n",
        "  style_reference_features = layer_features[1, :, :, :]\n",
        "  combination_features = layer_features[2, :, :, :]\n",
        "  sl = style_loss(style_reference_features, combination_features)\n",
        "  loss += (style_weight / len(style_layers)) * sl\n",
        "\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "\n",
        "grads = K.gradients(loss, combination_image)[0]\n",
        "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
        "\n",
        "class Evaluator(object):\n",
        "  def __init__(self):\n",
        "    self.loss_value = None\n",
        "    self.grads_values = None\n",
        "\n",
        "  def loss(self, x):\n",
        "    assert self.loss_value is None\n",
        "    x = x.reshape((1, img_height, img_width, 3))\n",
        "    outs = fetch_loss_and_grads([x])\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1].flatten().astype('float64')\n",
        "    self.loss_value = loss_value\n",
        "    self.grad_values = grad_values\n",
        "    return self.loss_value\n",
        "\n",
        "  def grads(self, x):\n",
        "    assert self.loss_value is not None\n",
        "    grad_values = np.copy(self.grad_values)\n",
        "    self.loss_value = None\n",
        "    self.grad_values = None\n",
        "    return grad_values\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "result_prefix = 'my_result'\n",
        "iterations = 20\n",
        "x = preprocess_image(target_image_path)\n",
        "x = x.flatten()\n",
        "\n",
        "for i in range(iterations):\n",
        "  print('Start of iteration', i)\n",
        "  start_time = time.time()\n",
        "  x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n",
        "                                   x,\n",
        "                                   fprime=evaluator.grads,\n",
        "                                   maxfun=20)\n",
        "  print('Current loss value:', min_val)\n",
        "  img = x.copy().reshape((img_height, img_width, 3))\n",
        "  img = deprocess_image(img)\n",
        "  fname = result_prefix + '_at_iteration_%d.png' % i\n",
        "  imageio.imwrite(fname, img)\n",
        "  print('Image saved as', fname)\n",
        "  end_time = time.time()\n",
        "  print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n",
            "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Start of iteration 0\n",
            "Current loss value: 11106227000.0\n",
            "Image saved as my_result_at_iteration_0.png\n",
            "Iteration 0 completed in 324s\n",
            "Start of iteration 1\n",
            "Current loss value: 3366980000.0\n",
            "Image saved as my_result_at_iteration_1.png\n",
            "Iteration 1 completed in 323s\n",
            "Start of iteration 2\n",
            "Current loss value: 1855401000.0\n",
            "Image saved as my_result_at_iteration_2.png\n",
            "Iteration 2 completed in 323s\n",
            "Start of iteration 3\n",
            "Current loss value: 1281516800.0\n",
            "Image saved as my_result_at_iteration_3.png\n",
            "Iteration 3 completed in 323s\n",
            "Start of iteration 4\n",
            "Current loss value: 1001861400.0\n",
            "Image saved as my_result_at_iteration_4.png\n",
            "Iteration 4 completed in 323s\n",
            "Start of iteration 5\n",
            "Current loss value: 867006340.0\n",
            "Image saved as my_result_at_iteration_5.png\n",
            "Iteration 5 completed in 322s\n",
            "Start of iteration 6\n",
            "Current loss value: 753733900.0\n",
            "Image saved as my_result_at_iteration_6.png\n",
            "Iteration 6 completed in 322s\n",
            "Start of iteration 7\n",
            "Current loss value: 673020740.0\n",
            "Image saved as my_result_at_iteration_7.png\n",
            "Iteration 7 completed in 322s\n",
            "Start of iteration 8\n",
            "Current loss value: 614587900.0\n",
            "Image saved as my_result_at_iteration_8.png\n",
            "Iteration 8 completed in 322s\n",
            "Start of iteration 9\n",
            "Current loss value: 568954400.0\n",
            "Image saved as my_result_at_iteration_9.png\n",
            "Iteration 9 completed in 321s\n",
            "Start of iteration 10\n",
            "Current loss value: 524010080.0\n",
            "Image saved as my_result_at_iteration_10.png\n",
            "Iteration 10 completed in 325s\n",
            "Start of iteration 11\n",
            "Current loss value: 492997800.0\n",
            "Image saved as my_result_at_iteration_11.png\n",
            "Iteration 11 completed in 322s\n",
            "Start of iteration 12\n",
            "Current loss value: 468169470.0\n",
            "Image saved as my_result_at_iteration_12.png\n",
            "Iteration 12 completed in 321s\n",
            "Start of iteration 13\n",
            "Current loss value: 445923600.0\n",
            "Image saved as my_result_at_iteration_13.png\n",
            "Iteration 13 completed in 323s\n",
            "Start of iteration 14\n",
            "Current loss value: 427805200.0\n",
            "Image saved as my_result_at_iteration_14.png\n",
            "Iteration 14 completed in 321s\n",
            "Start of iteration 15\n",
            "Current loss value: 410996260.0\n",
            "Image saved as my_result_at_iteration_15.png\n",
            "Iteration 15 completed in 322s\n",
            "Start of iteration 16\n",
            "Current loss value: 396611840.0\n",
            "Image saved as my_result_at_iteration_16.png\n",
            "Iteration 16 completed in 323s\n",
            "Start of iteration 17\n",
            "Current loss value: 378642020.0\n",
            "Image saved as my_result_at_iteration_17.png\n",
            "Iteration 17 completed in 322s\n",
            "Start of iteration 18\n",
            "Current loss value: 363509200.0\n",
            "Image saved as my_result_at_iteration_18.png\n",
            "Iteration 18 completed in 322s\n",
            "Start of iteration 19\n",
            "Current loss value: 349955360.0\n",
            "Image saved as my_result_at_iteration_19.png\n",
            "Iteration 19 completed in 323s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I644MDo87hyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ac8f3fd-e378-4891-8474-75d85eba259d"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "#z_mean, z_log_variance = encoder(input_img)\n",
        "#z = z_mean + exp(z_log_variance) * epsilon\n",
        "#reconstructed_img = decoder(z)\n",
        "#model = Model(input_img, reconstructed_img)\n",
        "\n",
        "img_shape = (28, 28, 1)\n",
        "batch_size = 16\n",
        "latent_dim = 2\n",
        "input_img = keras.Input(shape=img_shape)\n",
        "\n",
        "x = layers.Conv2D(32, 3, padding='same',\n",
        "                  activation='relu')(input_img)\n",
        "x = layers.Conv2D(64, 3, padding='same',\n",
        "                  activation='relu', strides=(2, 2))(x)\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "def sampling(args):\n",
        "  z_mean, z_log_var = args\n",
        "  epsilon = K.random_normal(shape=(K.shape(z_mean)[0],\n",
        "                                   latent_dim), mean=0., stddev=1.)\n",
        "  return z_mean + K.exp(z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
        "\n",
        "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
        "                 activation='relu')(decoder_input)\n",
        "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
        "x = layers.Conv2DTranspose(32, 3,\n",
        "                           padding='same',\n",
        "                           activation='relu',\n",
        "                           strides=(2, 2))(x)\n",
        "\n",
        "x = layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "decoder = Model(decoder_input, x)\n",
        "z_decoded = decoder(z)\n",
        "\n",
        "class CustomVariationalLayer(keras.layers.Layer):\n",
        "  def vae_loss(self, x, z_decoded):\n",
        "    x = K.flatten(x)\n",
        "    z_decoded = K.flatten(z_decoded)\n",
        "    xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "    kl_loss = -5e-4 * K.mean(\n",
        "    1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs[0]\n",
        "    z_decoded = inputs[1]\n",
        "    loss = self.vae_loss(x, z_decoded)\n",
        "    self.add_loss(loss, inputs=inputs)\n",
        "    return x\n",
        "\n",
        "y = CustomVariationalLayer()([input_img, z_decoded])\n",
        "\n",
        "vae = Model(input_img, y)\n",
        "vae.compile(optimizer='rmsprop', loss=None)\n",
        "vae.summary()\n",
        "\n",
        "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_test = x_test.reshape(x_test.shape + (1,))\n",
        "\n",
        "vae.fit(x=x_train,\n",
        "        y=None,\n",
        "        shuffle=True,\n",
        "        epochs=10,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, None))\n",
        "\n",
        "n = 15\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "  for j, xi in enumerate(grid_y):\n",
        "    z_sample = np.array([[xi, yi]])\n",
        "    z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
        "    x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
        "    digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "    figure[i * digit_size: (i + 1) * digit_size,\n",
        "           j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='Greys_r')\n",
        "plt.show()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 12544)        0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           401440      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            66          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Model)                 (None, 28, 28, 1)    56385       lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer_1 (Cus [(None, 28, 28, 1),  0           input_3[0][0]                    \n",
            "                                                                 model_1[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 550,629\n",
            "Trainable params: 550,629\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 381s 6ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 382s 6ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/10\n",
            "19328/60000 [========>.....................] - ETA: 4:07 - loss: nan"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6abd393f1aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         validation_data=(x_test, None))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJI1X3o19A5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6340303-cb3b-4575-ff36-a6c337040c0a"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "from keras import layers\n",
        "from keras.preprocessing import image\n",
        "\n",
        "latent_dim = 32\n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "\n",
        "generator_input = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Reshape((16, 16, 128))(x)\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(256, 5, padding='same')(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
        "\n",
        "generator = keras.models.Model(generator_input, x)\n",
        "generator.summary()\n",
        "\n",
        "discriminator_input = layers.Input(shape=(height, width, channels))\n",
        "\n",
        "x = layers.Conv2D(128, 3)(discriminator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "discriminator = keras.models.Model(discriminator_input, x)\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008,\n",
        "                                                   clipvalue=1.0,\n",
        "                                                   decay=1e-8)\n",
        "\n",
        "discriminator.compile(optimizer=discriminator_optimizer,\n",
        "                      loss='binary_crossentropy')\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = keras.Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = keras.models.Model(gan_input, gan_output)\n",
        "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
        "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')\n",
        "\n",
        "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train[y_train.flatten() == 6]\n",
        "x_train = x_train.reshape((x_train.shape[0],) +\n",
        "                          (height, width, channels)).astype('float32') / 255.\n",
        "\n",
        "iterations = 10000\n",
        "batch_size = 20\n",
        "save_dir = ''\n",
        "start = 0\n",
        "\n",
        "for step in range(iterations):\n",
        "  random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "  generated_images = generator.predict(random_latent_vectors)\n",
        "  stop = start + batch_size\n",
        "  real_images = x_train[start: stop]\n",
        "  combined_images = np.concatenate([generated_images, real_images])\n",
        "  labels = np.concatenate([np.ones((batch_size, 1)),\n",
        "                           np.zeros((batch_size, 1))])\n",
        "  labels += 0.05 * np.random.random(labels.shape)\n",
        "  d_loss = discriminator.train_on_batch(combined_images, labels)\n",
        "  random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
        "  misleading_targets = np.zeros((batch_size, 1))\n",
        "  a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
        "  start += batch_size\n",
        "\n",
        "  if start > len(x_train) - batch_size:\n",
        "    start = 0\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    gan.save_weights('gan.h5')\n",
        "    print('discriminator loss:', d_loss)\n",
        "    print('adversarial loss:', a_loss)\n",
        "    img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
        "    img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
        "    img = image.array_to_img(real_images[0] * 255., scale=False)\n",
        "    img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 32768)             1081344   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 16, 16, 256)       819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 32, 32, 256)       1638656   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 32, 32, 3)         37635     \n",
            "=================================================================\n",
            "Total params: 6,264,579\n",
            "Trainable params: 6,264,579\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 30, 30, 128)       3584      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 30, 30, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 14, 14, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 6, 6, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 2, 2, 128)         262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 790,913\n",
            "Trainable params: 790,913\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "discriminator loss: 0.68157256\n",
            "adversarial loss: 0.273564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbIuyf3oCS9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}